{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7768fd51-9f7b-4fd1-89f7-ce9bb3043009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75c90a7d-5da1-483e-bae3-efadce722ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from scikeras) (3.11.3)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from scikeras) (1.4.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from keras>=3.2.0->scikeras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from keras>=3.2.0->scikeras) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.5.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from keras>=3.2.0->scikeras) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from optree->keras>=3.2.0->scikeras) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\johanna\\anaconda3\\envs\\ai_project\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41a6217c-7087-4837-8f49-17e63cf13968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from tensorflow.keras.layers import  Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from joblib import parallel_backend\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aabf74bb-a1c9-4222-a454-9b31c8d8a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_lagged_all = pd.read_parquet(\"data/processed/df_lagged_30_all.parquet\")\n",
    "df_lagged_all = pd.read_parquet(\"data/processed/df_lagged_all.parquet\")\n",
    "\n",
    "df_2_full = pd.read_parquet(\"data/processed/df_2_full_v2.parquet\")\n",
    "df_all=pd.read_parquet(\"data/processed/df_all.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac797839-78ec-4f39-b417-f351d8b13d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sensor = 1076\n",
    "same_portal=\"55620\"\n",
    "neighbour_portal = \"56160\"\n",
    "\n",
    "same_portal_sensors = df_2_full[df_2_full['PORTAL_clean'] == same_portal]['DP_ID'].unique()\n",
    "same_sensors = [s for s in same_portal_sensors if s != target_sensor]\n",
    "\n",
    "\n",
    "neighbour_sensors = df_2_full[df_2_full['PORTAL_clean'] == neighbour_portal]['DP_ID'].unique()\n",
    "all_sensors=df_2_full['DP_ID'].unique()\n",
    "except_target_sensors = [s for s in all_sensors if s != target_sensor]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f5972f6-5842-446d-8b73-a14928be0a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlags=15\n",
    "#nlags=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c27edb9-8c27-4658-8c63-b99c6a27ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "flow_features_same = [\n",
    "    f'SENSOR_{sensor}_FLOW_lag_{i+1}'\n",
    "    for sensor in same_sensors\n",
    "    for i in range(nlags)\n",
    "]\n",
    "flow_features_neighbour = [\n",
    "    f'SENSOR_{sensor}_FLOW_lag_{i+1}'\n",
    "    for sensor in neighbour_sensors\n",
    "    for i in range(nlags)\n",
    "]\n",
    "speed_features_same = [\n",
    "    f'SENSOR_{sensor}_SPEED_lag_{i+1}'\n",
    "    for sensor in same_sensors\n",
    "    for i in range(nlags)\n",
    "]\n",
    "speed_features_neighbour = [\n",
    "    f'SENSOR_{sensor}_SPEED_lag_{i+1}'\n",
    "    for sensor in neighbour_sensors\n",
    "    for i in range(nlags)\n",
    "]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2466b25-b0cf-45cb-b70b-70174bae1b91",
   "metadata": {},
   "source": [
    "xgb-boost : hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a65de289-ff0f-4bbc-bca0-7a623f17634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_lagged_all, test_size=0.2, random_state=42)\n",
    "\n",
    "#split_time = df_2_full['Datetime'].quantile(0.8)\n",
    "#print(split_time)\n",
    "\n",
    "#df_train = df_lagged_all[df_lagged_all['Datetime'] <= split_time]\n",
    "#df_test = df_lagged_all[df_lagged_all['Datetime'] > split_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf3a3b-e25d-425b-bd55-9a52baf9e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.001,0.005,0.01, 0.05],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b359e1cf-6971-4402-8351-98cabfaa939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Beste Parameter (FLOW): {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "XGBoost Regression (FLOW) -> RMSE: 28.452, MAE: 20.456, R²: 0.882\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter (FLOW): {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
      "XGBoost Regression (SPEED) -> RMSE: 0.813, MAE: 0.398, R²: 0.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "random_search_flow = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,  # Anzahl zufälliger Kombinationen\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-2\n",
    ")\n",
    "\n",
    "random_search_flow.fit(df_train[flow_features_same], df_train[\"FLOW_future_sum\"])\n",
    "print(\"Beste Parameter (FLOW):\", random_search_flow.best_params_)\n",
    "model_flow = random_search_flow.best_estimator_\n",
    "y_pred = model_flow.predict(df_test[flow_features_same])\n",
    "y_test = df_test['FLOW_future_sum']\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost Regression (FLOW) -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n",
    "\n",
    "random_search_speed = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,  # Anzahl zufälliger Kombinationen\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-2\n",
    ")\n",
    "\n",
    "\n",
    "random_search_speed.fit(df_train[speed_features_same], df_train[\"SPEED_future_mean\"])\n",
    "print(\"Beste Parameter (FLOW):\", random_search_speed.best_params_)\n",
    "model_speed = random_search_speed.best_estimator_\n",
    "\n",
    "y_pred = model_speed.predict(df_test[speed_features_same])\n",
    "y_test = df_test['SPEED_future_mean']\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost Regression (SPEED) -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c16d6f1-4dd1-4fac-a0d5-19846c3a7caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Beste Parameter (FLOW): {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "XGBoost Regression (FLOW) -> RMSE: 24.096, MAE: 16.871, R²: 0.915\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter (FLOW): {'subsample': 0.6, 'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "XGBoost Regression (SPEED) -> RMSE: 0.934, MAE: 0.421, R²: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#neighbour portal\n",
    "\n",
    "random_search_flow = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,  # Anzahl zufälliger Kombinationen\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-2\n",
    ")\n",
    "\n",
    "random_search_flow.fit(df_train[flow_features_neighbour], df_train[\"FLOW_future_sum\"])\n",
    "print(\"Beste Parameter (FLOW):\", random_search_flow.best_params_)\n",
    "model_flow = random_search_flow.best_estimator_\n",
    "y_pred = model_flow.predict(df_test[flow_features_neighbour])\n",
    "y_test = df_test['FLOW_future_sum']\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost Regression (FLOW) -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n",
    "\n",
    "random_search_speed = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,  # Anzahl zufälliger Kombinationen\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-2\n",
    ")\n",
    "\n",
    "\n",
    "random_search_speed.fit(df_train[speed_features_neighbour], df_train[\"SPEED_future_mean\"])\n",
    "print(\"Beste Parameter (FLOW):\", random_search_speed.best_params_)\n",
    "model_speed = random_search_speed.best_estimator_\n",
    "\n",
    "y_pred = model_speed.predict(df_test[speed_features_neighbour])\n",
    "y_test = df_test['SPEED_future_mean']\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"XGBoost Regression (SPEED) -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f597a15-b86d-4f1f-9416-95e166f15ad4",
   "metadata": {},
   "source": [
    "Feedforward Neural Network (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "14177efa-8919-49c5-9c1c-5b69343236dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step\n",
      "Fast forward NN (FLOW) -> RMSE: 29.059, MAE: 20.701, R²: 0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step\n",
      "Fast forward NN (SPEED) -> RMSE: 0.800, MAE: 0.416, R²: 0.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_train[flow_features_same])\n",
    "X_test_scaled  = scaler.transform(df_test[flow_features_same])\n",
    "\n",
    "\n",
    "model_flow= Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Eingabeschicht\n",
    "    Dense(32, activation='relu'),                                           # Versteckte Schicht\n",
    "    Dense(16, activation='relu'),                                           # Weitere versteckte Schicht\n",
    "    Dense(1)                                                                 # Ausgangsschicht für Regression\n",
    "])\n",
    "\n",
    "model_flow.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "\n",
    "history_flow = model_flow.fit(\n",
    "    X_train_scaled, df_train[\"FLOW_future_sum\"],\n",
    "    validation_split=0.2,  # 20% der Trainingsdaten für Validierung\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model_flow.predict(X_test_scaled)\n",
    "y_test=df_test[\"FLOW_future_sum\"]\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Fast forward NN (FLOW) -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_train[speed_features_same])\n",
    "X_test_scaled  = scaler.transform(df_test[speed_features_same])\n",
    "\n",
    "model_speed = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Eingabeschicht\n",
    "    Dense(32, activation='relu'),                                           # Versteckte Schicht\n",
    "    Dense(16, activation='relu'),                                           # Weitere versteckte Schicht\n",
    "    Dense(1)                                                                 # Ausgangsschicht für Regression\n",
    "])\n",
    "\n",
    "model_speed.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "\n",
    "history_speed = model_speed.fit(\n",
    "    X_train_scaled, df_train[\"SPEED_future_mean\"],\n",
    "    validation_split=0.2,  # 20% der Trainingsdaten für Validierung\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "y_pred = model_speed.predict(X_test_scaled)\n",
    "y_test=df_test[\"SPEED_future_mean\"]\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Fast forward NN (SPEED) -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca849db4-1240-4f02-947c-5ea3869862ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step\n",
      "Fast forward NN (FLOW) -> RMSE: 25.035, MAE: 17.839, R²: 0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step\n",
      "Fast forward NN (SPEED) -> RMSE: 0.954, MAE: 0.438, R²: 0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_train[flow_features_neighbour])\n",
    "X_test_scaled  = scaler.transform(df_test[flow_features_neighbour])\n",
    "\n",
    "\n",
    "model_flow= Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Eingabeschicht\n",
    "    Dense(32, activation='relu'),                                           # Versteckte Schicht\n",
    "    Dense(16, activation='relu'),                                           # Weitere versteckte Schicht\n",
    "    Dense(1)                                                                 # Ausgangsschicht für Regression\n",
    "])\n",
    "\n",
    "model_flow.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "\n",
    "history_flow = model_flow.fit(\n",
    "    X_train_scaled, df_train[\"FLOW_future_sum\"],\n",
    "    validation_split=0.2,  # 20% der Trainingsdaten für Validierung\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model_flow.predict(X_test_scaled)\n",
    "y_test=df_test[\"FLOW_future_sum\"]\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Fast forward NN (FLOW) -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_train[speed_features_neighbour])\n",
    "X_test_scaled  = scaler.transform(df_test[speed_features_neighbour])\n",
    "\n",
    "model_speed = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Eingabeschicht\n",
    "    Dense(32, activation='relu'),                                           # Versteckte Schicht\n",
    "    Dense(16, activation='relu'),                                           # Weitere versteckte Schicht\n",
    "    Dense(1)                                                                 # Ausgangsschicht für Regression\n",
    "])\n",
    "\n",
    "model_speed.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "\n",
    "history_speed = model_speed.fit(\n",
    "    X_train_scaled, df_train[\"SPEED_future_mean\"],\n",
    "    validation_split=0.2,  # 20% der Trainingsdaten für Validierung\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "y_pred = model_speed.predict(X_test_scaled)\n",
    "y_test=df_test[\"SPEED_future_mean\"]\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Fast forward NN (SPEED) -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d48647a9-4747-4637-a548-8335a8094185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\AppData\\Local\\Temp\\ipykernel_10492\\308595228.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  grid_search_df = pd.concat([grid_search_df, it_df], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2 0.0 done\n",
      "16 2 0.05 done\n",
      "16 2 0.1 done\n",
      "16 3 0.0 done\n",
      "16 3 0.05 done\n",
      "16 3 0.1 done\n",
      "16 4 0.0 done\n",
      "16 4 0.05 done\n",
      "16 4 0.1 done\n",
      "32 2 0.0 done\n",
      "32 2 0.05 done\n",
      "32 2 0.1 done\n",
      "32 3 0.0 done\n",
      "32 3 0.05 done\n",
      "32 3 0.1 done\n",
      "32 4 0.0 done\n",
      "32 4 0.05 done\n",
      "32 4 0.1 done\n",
      "64 2 0.0 done\n",
      "64 2 0.05 done\n",
      "64 2 0.1 done\n",
      "64 3 0.0 done\n",
      "64 3 0.05 done\n",
      "64 3 0.1 done\n",
      "64 4 0.0 done\n",
      "64 4 0.05 done\n",
      "64 4 0.1 done\n",
      "128 2 0.0 done\n",
      "128 2 0.05 done\n",
      "128 2 0.1 done\n",
      "128 3 0.0 done\n",
      "128 3 0.05 done\n",
      "128 3 0.1 done\n",
      "128 4 0.0 done\n",
      "128 4 0.05 done\n",
      "128 4 0.1 done\n",
      "   number_neurons number_layers  dropout_rate  train_rmse   val_rmse\n",
      "0              16             2          0.00   29.371403  29.733335\n",
      "1              16             2          0.05   37.075043  33.153324\n",
      "2              16             2          0.10   42.041279  33.635246\n",
      "3              16             3          0.00   27.912714  28.976200\n",
      "4              16             3          0.05   39.897415  33.503639\n",
      "5              16             3          0.10   43.846443  34.295017\n",
      "6              16             4          0.00   28.486589  29.510708\n",
      "7              16             4          0.05   36.866894  31.401754\n",
      "8              16             4          0.10   40.082947  31.197016\n",
      "9              32             2          0.00   28.014595  29.070684\n",
      "10             32             2          0.05   32.399998  30.271238\n",
      "11             32             2          0.10   34.075420  30.764746\n",
      "12             32             3          0.00   27.727701  29.272102\n",
      "13             32             3          0.05   32.647373  30.021769\n",
      "14             32             3          0.10   35.011250  31.629435\n",
      "15             32             4          0.00   28.093256  29.142313\n",
      "16             32             4          0.05   31.315088  29.387403\n",
      "17             32             4          0.10   35.715744  31.192465\n",
      "18             64             2          0.00   27.472795  29.341480\n",
      "19             64             2          0.05   32.532330  30.811172\n",
      "20             64             2          0.10   33.490093  30.293657\n",
      "21             64             3          0.00   28.851919  29.869560\n",
      "22             64             3          0.05   30.222097  29.404978\n",
      "23             64             3          0.10   31.837008  29.717525\n",
      "24             64             4          0.00   27.025183  28.974485\n",
      "25             64             4          0.05   30.128540  29.311167\n",
      "26             64             4          0.10   32.301857  29.614431\n",
      "27            128             2          0.00   27.893883  29.641785\n",
      "28            128             2          0.05   30.282890  29.778042\n",
      "29            128             2          0.10   31.468235  30.192453\n",
      "30            128             3          0.00   27.376104  29.253227\n",
      "31            128             3          0.05   28.109545  28.893290\n",
      "32            128             3          0.10   28.693462  29.138802\n",
      "33            128             4          0.00   28.150019  29.234877\n",
      "34            128             4          0.05   29.346590  29.468685\n",
      "35            128             4          0.10   30.588945  29.653276\n",
      "Epoch 1/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 891us/step - loss: 1731.5171 - rmse: 41.6115 - val_loss: 1050.1238 - val_rmse: 32.4056 - learning_rate: 0.0100\n",
      "Epoch 2/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step - loss: 1095.2737 - rmse: 33.0949 - val_loss: 1041.9083 - val_rmse: 32.2786 - learning_rate: 0.0100\n",
      "Epoch 3/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 921us/step - loss: 1052.7274 - rmse: 32.4458 - val_loss: 964.5223 - val_rmse: 31.0568 - learning_rate: 0.0100\n",
      "Epoch 4/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 812us/step - loss: 1027.7020 - rmse: 32.0578 - val_loss: 976.1412 - val_rmse: 31.2433 - learning_rate: 0.0100\n",
      "Epoch 5/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 994.6746 - rmse: 31.5385 - val_loss: 1033.4736 - val_rmse: 32.1477 - learning_rate: 0.0100\n",
      "Epoch 6/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step - loss: 981.7228 - rmse: 31.3325 - val_loss: 985.1785 - val_rmse: 31.3876 - learning_rate: 0.0100\n",
      "Epoch 7/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 818us/step - loss: 914.6654 - rmse: 30.2434 - val_loss: 898.3356 - val_rmse: 29.9722 - learning_rate: 0.0050\n",
      "Epoch 8/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809us/step - loss: 913.1998 - rmse: 30.2192 - val_loss: 908.3728 - val_rmse: 30.1392 - learning_rate: 0.0050\n",
      "Epoch 9/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805us/step - loss: 905.7272 - rmse: 30.0953 - val_loss: 921.6393 - val_rmse: 30.3585 - learning_rate: 0.0050\n",
      "Epoch 10/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893us/step - loss: 898.9813 - rmse: 29.9830 - val_loss: 892.2537 - val_rmse: 29.8706 - learning_rate: 0.0050\n",
      "Epoch 11/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 818us/step - loss: 899.8989 - rmse: 29.9983 - val_loss: 1066.1420 - val_rmse: 32.6518 - learning_rate: 0.0050\n",
      "Epoch 12/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step - loss: 887.7765 - rmse: 29.7956 - val_loss: 887.4865 - val_rmse: 29.7907 - learning_rate: 0.0050\n",
      "Epoch 13/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step - loss: 885.9044 - rmse: 29.7641 - val_loss: 896.2809 - val_rmse: 29.9380 - learning_rate: 0.0050\n",
      "Epoch 14/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809us/step - loss: 870.7873 - rmse: 29.5091 - val_loss: 896.1351 - val_rmse: 29.9355 - learning_rate: 0.0050\n",
      "Epoch 15/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 815us/step - loss: 866.4787 - rmse: 29.4360 - val_loss: 885.2903 - val_rmse: 29.7538 - learning_rate: 0.0050\n",
      "Epoch 16/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 820us/step - loss: 847.0215 - rmse: 29.1036 - val_loss: 859.8157 - val_rmse: 29.3226 - learning_rate: 0.0050\n",
      "Epoch 17/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 886us/step - loss: 828.8560 - rmse: 28.7899 - val_loss: 861.1297 - val_rmse: 29.3450 - learning_rate: 0.0050\n",
      "Epoch 18/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 817.4312 - rmse: 28.5908 - val_loss: 861.0536 - val_rmse: 29.3437 - learning_rate: 0.0050\n",
      "Epoch 19/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step - loss: 811.1301 - rmse: 28.4803 - val_loss: 910.0776 - val_rmse: 30.1675 - learning_rate: 0.0050\n",
      "Epoch 20/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 776.3370 - rmse: 27.8628 - val_loss: 855.3643 - val_rmse: 29.2466 - learning_rate: 0.0025\n",
      "Epoch 21/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 767.4539 - rmse: 27.7030 - val_loss: 882.0974 - val_rmse: 29.7001 - learning_rate: 0.0025\n",
      "Epoch 22/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 762.5419 - rmse: 27.6142 - val_loss: 866.0720 - val_rmse: 29.4291 - learning_rate: 0.0025\n",
      "Epoch 23/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 759.6696 - rmse: 27.5621 - val_loss: 854.6257 - val_rmse: 29.2340 - learning_rate: 0.0025\n",
      "Epoch 24/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 806us/step - loss: 753.4178 - rmse: 27.4485 - val_loss: 866.6935 - val_rmse: 29.4397 - learning_rate: 0.0025\n",
      "Epoch 25/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817us/step - loss: 753.2640 - rmse: 27.4457 - val_loss: 858.0863 - val_rmse: 29.2931 - learning_rate: 0.0025\n",
      "Epoch 26/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840us/step - loss: 745.8684 - rmse: 27.3106 - val_loss: 891.2662 - val_rmse: 29.8541 - learning_rate: 0.0025\n",
      "Epoch 27/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 724.6740 - rmse: 26.9198 - val_loss: 866.9536 - val_rmse: 29.4441 - learning_rate: 0.0012\n",
      "Epoch 28/100\n",
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 720.0946 - rmse: 26.8346 - val_loss: 870.4365 - val_rmse: 29.5032 - learning_rate: 0.0012\n",
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step\n",
      "Best FLOW NN -> RMSE: 28.720, MAE: 20.481, R²: 0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#same portal-flow\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_train[flow_features_same])\n",
    "X_test_scaled  = scaler.transform(df_test[flow_features_same])\n",
    "y_train = df_train[\"FLOW_future_sum\"]\n",
    "y_test = df_test[\"FLOW_future_sum\"]\n",
    "\n",
    "number_neurons = [16, 32, 64, 128]\n",
    "number_layers = [2, 3,4]\n",
    "dropout_rates = [0.0, 0.05, 0.1]\n",
    "\n",
    "grid_search_df = pd.DataFrame(columns=[\"number_neurons\",\"number_layers\",\"dropout_rate\",\"train_rmse\", \"val_rmse\"])\n",
    "\n",
    "for neurons in number_neurons:\n",
    "    for num_layer in number_layers:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            model = Sequential()\n",
    "            for _ in range(num_layer):\n",
    "                model.add(Dense(neurons, activation='relu'))\n",
    "                if dropout_rate > 0:\n",
    "                    model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(1))\n",
    "            optimizer = Adam(learning_rate=0.01)\n",
    "            model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    \n",
    "            early_stop = EarlyStopping(monitor='val_rmse', patience=5, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3)\n",
    "    \n",
    "            # Optional: speichern der besten Gewichte\n",
    "            #checkpoint = ModelCheckpoint(\"model/NN_model_flow_same_best.keras\", monitor='val_rmse', save_best_only=True, mode='min')\n",
    "    \n",
    "            hist = model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                validation_split=0.2,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stop, reduce_lr],\n",
    "                verbose=0\n",
    "            )\n",
    "    \n",
    "            # Ergebnisse speichern\n",
    "            min_err = min(hist.history['rmse'])\n",
    "            min_val_err = min(hist.history['val_rmse'])\n",
    "            it_df = pd.DataFrame({\n",
    "                \"number_neurons\": [neurons],\n",
    "                \"number_layers\": [num_layer],\n",
    "                \"dropout_rate\":[dropout_rate],\n",
    "                \"train_rmse\": [min_err],\n",
    "                \"val_rmse\": [min_val_err]\n",
    "            })\n",
    "            grid_search_df = pd.concat([grid_search_df, it_df], axis=0)\n",
    "            print(neurons, num_layer,dropout_rate, \"done\")\n",
    "\n",
    "# Übersicht\n",
    "grid_search_df.reset_index(drop=True, inplace=True)\n",
    "print(grid_search_df)\n",
    "best_config = grid_search_df.loc[grid_search_df['val_rmse'].idxmin()]\n",
    "best_neurons = int(best_config['number_neurons'])\n",
    "best_layers = int(best_config['number_layers'])\n",
    "best_dropout = int(best_config['dropout_rate'])\n",
    "\n",
    "# Modell mit besten Parametern\n",
    "best_model = Sequential()\n",
    "for _ in range(best_layers):\n",
    "    best_model.add(Dense(best_neurons, activation='relu'))\n",
    "    if best_dropout > 0:\n",
    "        best_model.add(Dropout(best_dropout))\n",
    "best_model.add(Dense(1))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "best_model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_rmse', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3)\n",
    "checkpoint = ModelCheckpoint(\"model/NN_model_flow_same_best.keras\", monitor='val_rmse', save_best_only=True, mode='min')\n",
    "\n",
    "# Training\n",
    "best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Testbewertung\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best FLOW NN -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26e8c8c5-50a9-425d-8f1f-b9b940d4e7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\AppData\\Local\\Temp\\ipykernel_10492\\867997906.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  grid_search_df = pd.concat([grid_search_df, it_df], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2 0.0 done\n",
      "16 2 0.05 done\n",
      "16 2 0.1 done\n",
      "16 3 0.0 done\n",
      "16 3 0.05 done\n",
      "16 3 0.1 done\n",
      "16 4 0.0 done\n",
      "16 4 0.05 done\n",
      "16 4 0.1 done\n",
      "32 2 0.0 done\n",
      "32 2 0.05 done\n",
      "32 2 0.1 done\n",
      "32 3 0.0 done\n",
      "32 3 0.05 done\n",
      "32 3 0.1 done\n",
      "32 4 0.0 done\n",
      "32 4 0.05 done\n",
      "32 4 0.1 done\n",
      "64 2 0.0 done\n",
      "64 2 0.05 done\n",
      "64 2 0.1 done\n",
      "64 3 0.0 done\n",
      "64 3 0.05 done\n",
      "64 3 0.1 done\n",
      "64 4 0.0 done\n",
      "64 4 0.05 done\n",
      "64 4 0.1 done\n",
      "128 2 0.0 done\n",
      "128 2 0.05 done\n",
      "128 2 0.1 done\n",
      "128 3 0.0 done\n",
      "128 3 0.05 done\n",
      "128 3 0.1 done\n",
      "128 4 0.0 done\n",
      "128 4 0.05 done\n",
      "128 4 0.1 done\n",
      "   number_neurons number_layers  dropout_rate  train_rmse  val_rmse\n",
      "0              16             2          0.00    0.805148  0.790606\n",
      "1              16             2          0.05    0.973746  0.811842\n",
      "2              16             2          0.10    0.822249  0.793180\n",
      "3              16             3          0.00    0.733872  0.737610\n",
      "4              16             3          0.05    0.812134  0.754808\n",
      "5              16             3          0.10    0.878491  0.784833\n",
      "6              16             4          0.00    0.777013  0.766441\n",
      "7              16             4          0.05    0.817865  0.782449\n",
      "8              16             4          0.10    0.857271  0.780539\n",
      "9              32             2          0.00    0.718395  0.731156\n",
      "10             32             2          0.05    0.779175  0.755556\n",
      "11             32             2          0.10    0.830919  0.771503\n",
      "12             32             3          0.00    0.753205  0.745721\n",
      "13             32             3          0.05    0.945618  0.791097\n",
      "14             32             3          0.10    0.839426  0.774840\n",
      "15             32             4          0.00    0.817331  0.776057\n",
      "16             32             4          0.05    0.826668  0.770087\n",
      "17             32             4          0.10    0.865351  0.774622\n",
      "18             64             2          0.00    0.726241  0.732919\n",
      "19             64             2          0.05    0.775880  0.740514\n",
      "20             64             2          0.10    0.784956  0.756553\n",
      "21             64             3          0.00    0.869865  0.817201\n",
      "22             64             3          0.05    0.809672  0.757874\n",
      "23             64             3          0.10    0.927092  0.805171\n",
      "24             64             4          0.00    0.888662  0.800578\n",
      "25             64             4          0.05    0.826923  0.755538\n",
      "26             64             4          0.10    1.077974  0.812046\n",
      "27            128             2          0.00    0.775472  0.756554\n",
      "28            128             2          0.05    0.764023  0.731291\n",
      "29            128             2          0.10    0.917648  0.782089\n",
      "30            128             3          0.00    0.744699  0.737121\n",
      "31            128             3          0.05    0.859786  0.754419\n",
      "32            128             3          0.10    0.862830  0.780701\n",
      "33            128             4          0.00    0.793643  0.794060\n",
      "34            128             4          0.05    1.077041  0.858270\n",
      "35            128             4          0.10    0.951876  0.796824\n",
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step\n",
      "Best SPEED NN -> RMSE: 0.805, MAE: 0.419, R²: 0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#same portal-speed\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_train[speed_features_same])\n",
    "X_test_scaled  = scaler.transform(df_test[speed_features_same])\n",
    "y_train = df_train[\"SPEED_future_mean\"]\n",
    "y_test = df_test[\"SPEED_future_mean\"]\n",
    "\n",
    "number_neurons = [16, 32, 64, 128]\n",
    "number_layers = [2, 3,4]\n",
    "dropout_rates = [0.0, 0.05, 0.1]\n",
    "\n",
    "grid_search_df = pd.DataFrame(columns=[\"number_neurons\",\"number_layers\",\"dropout_rate\",\"train_rmse\", \"val_rmse\"])\n",
    "\n",
    "for neurons in number_neurons:\n",
    "    for num_layer in number_layers:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            model = Sequential()\n",
    "            for _ in range(num_layer):\n",
    "                model.add(Dense(neurons, activation='relu'))\n",
    "                if dropout_rate > 0:\n",
    "                    model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(1))\n",
    "            optimizer = Adam(learning_rate=0.01)\n",
    "            model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    \n",
    "            early_stop = EarlyStopping(monitor='val_rmse', patience=5, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3)\n",
    "    \n",
    "            # Optional: speichern der besten Gewichte\n",
    "            #checkpoint = ModelCheckpoint(\"model/NN_model_speed_same_best.keras\", monitor='val_rmse', save_best_only=True, mode='min')\n",
    "    \n",
    "            hist = model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                validation_split=0.2,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stop, reduce_lr],\n",
    "                verbose=0\n",
    "            )\n",
    "    \n",
    "            # Ergebnisse speichern\n",
    "            min_err = min(hist.history['rmse'])\n",
    "            min_val_err = min(hist.history['val_rmse'])\n",
    "            it_df = pd.DataFrame({\n",
    "                \"number_neurons\": [neurons],\n",
    "                \"number_layers\": [num_layer],\n",
    "                \"dropout_rate\":[dropout_rate],\n",
    "                \"train_rmse\": [min_err],\n",
    "                \"val_rmse\": [min_val_err]\n",
    "            })\n",
    "            grid_search_df = pd.concat([grid_search_df, it_df], axis=0)\n",
    "            print(neurons, num_layer,dropout_rate, \"done\")\n",
    "\n",
    "# Übersicht\n",
    "grid_search_df.reset_index(drop=True, inplace=True)\n",
    "print(grid_search_df)\n",
    "best_config = grid_search_df.loc[grid_search_df['val_rmse'].idxmin()]\n",
    "best_neurons = int(best_config['number_neurons'])\n",
    "best_layers = int(best_config['number_layers'])\n",
    "best_dropout = int(best_config['dropout_rate'])\n",
    "\n",
    "# Modell mit besten Parametern\n",
    "best_model = Sequential()\n",
    "for _ in range(best_layers):\n",
    "    best_model.add(Dense(best_neurons, activation='relu'))\n",
    "    if best_dropout > 0:\n",
    "        best_model.add(Dropout(best_dropout))\n",
    "best_model.add(Dense(1))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "best_model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_rmse', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3)\n",
    "checkpoint = ModelCheckpoint(\"model/NN_model_speed_same_best.keras\", monitor='val_rmse', save_best_only=True, mode='min')\n",
    "\n",
    "# Training\n",
    "best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Testbewertung\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best SPEED NN -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "22653b62-c1f5-490b-9496-87c46238288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\AppData\\Local\\Temp\\ipykernel_10492\\2257366272.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  grid_search_df = pd.concat([grid_search_df, it_df], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2 0.0 done\n",
      "16 2 0.05 done\n",
      "16 2 0.1 done\n",
      "16 3 0.0 done\n",
      "16 3 0.05 done\n",
      "16 3 0.1 done\n",
      "16 4 0.0 done\n",
      "16 4 0.05 done\n",
      "16 4 0.1 done\n",
      "32 2 0.0 done\n",
      "32 2 0.05 done\n",
      "32 2 0.1 done\n",
      "32 3 0.0 done\n",
      "32 3 0.05 done\n",
      "32 3 0.1 done\n",
      "32 4 0.0 done\n",
      "32 4 0.05 done\n",
      "32 4 0.1 done\n",
      "64 2 0.0 done\n",
      "64 2 0.05 done\n",
      "64 2 0.1 done\n",
      "64 3 0.0 done\n",
      "64 3 0.05 done\n",
      "64 3 0.1 done\n",
      "64 4 0.0 done\n",
      "64 4 0.05 done\n",
      "64 4 0.1 done\n",
      "128 2 0.0 done\n",
      "128 2 0.05 done\n",
      "128 2 0.1 done\n",
      "128 3 0.0 done\n",
      "128 3 0.05 done\n",
      "128 3 0.1 done\n",
      "128 4 0.0 done\n",
      "128 4 0.05 done\n",
      "128 4 0.1 done\n",
      "   number_neurons number_layers  dropout_rate  train_rmse   val_rmse\n",
      "0              16             2          0.00   24.768108  25.254890\n",
      "1              16             2          0.05   29.870770  26.075983\n",
      "2              16             2          0.10   33.876259  25.873125\n",
      "3              16             3          0.00   23.947803  24.581074\n",
      "4              16             3          0.05   32.065853  28.567739\n",
      "5              16             3          0.10   34.205040  26.713118\n",
      "6              16             4          0.00   24.628374  25.154072\n",
      "7              16             4          0.05   31.729382  27.132214\n",
      "8              16             4          0.10   33.573238  27.741846\n",
      "9              32             2          0.00   23.871603  24.740385\n",
      "10             32             2          0.05   28.478031  25.412991\n",
      "11             32             2          0.10   29.893749  25.724424\n",
      "12             32             3          0.00   23.454884  24.592428\n",
      "13             32             3          0.05   27.005396  24.729988\n",
      "14             32             3          0.10   28.438347  24.731655\n",
      "15             32             4          0.00   24.006987  24.754995\n",
      "16             32             4          0.05   29.598333  25.834652\n",
      "17             32             4          0.10   31.037954  25.666662\n",
      "18             64             2          0.00   23.552530  24.551559\n",
      "19             64             2          0.05   27.162569  25.262213\n",
      "20             64             2          0.10   30.057209  26.172491\n",
      "21             64             3          0.00   23.170532  24.796612\n",
      "22             64             3          0.05   25.607405  24.497496\n",
      "23             64             3          0.10   26.509783  24.817190\n",
      "24             64             4          0.00   23.471045  24.647360\n",
      "25             64             4          0.05   26.190321  24.921551\n",
      "26             64             4          0.10   30.408360  27.010319\n",
      "27            128             2          0.00   23.614721  24.654772\n",
      "28            128             2          0.05   25.052713  24.795582\n",
      "29            128             2          0.10   27.073999  25.192635\n",
      "30            128             3          0.00   23.714266  25.001190\n",
      "31            128             3          0.05   24.348314  24.660208\n",
      "32            128             3          0.10   24.832069  24.943247\n",
      "33            128             4          0.00   23.297321  24.700212\n",
      "34            128             4          0.05   26.094875  25.349012\n",
      "35            128             4          0.10   30.004120  26.820402\n",
      "64 3 0\n",
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step\n",
      "Best FLOW NN -> RMSE: 24.511, MAE: 17.286, R²: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#neighbour portal-flow\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_train[flow_features_neighbour])\n",
    "X_test_scaled  = scaler.transform(df_test[flow_features_neighbour])\n",
    "y_train = df_train[\"FLOW_future_sum\"]\n",
    "y_test = df_test[\"FLOW_future_sum\"]\n",
    "\n",
    "number_neurons = [16, 32, 64, 128]\n",
    "number_layers = [2, 3,4]\n",
    "dropout_rates = [0.0, 0.05, 0.1]\n",
    "\n",
    "grid_search_df = pd.DataFrame(columns=[\"number_neurons\",\"number_layers\",\"dropout_rate\",\"train_rmse\", \"val_rmse\"])\n",
    "\n",
    "for neurons in number_neurons:\n",
    "    for num_layer in number_layers:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            model = Sequential()\n",
    "            for _ in range(num_layer):\n",
    "                model.add(Dense(neurons, activation='relu'))\n",
    "                if dropout_rate > 0:\n",
    "                    model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(1))\n",
    "            optimizer = Adam(learning_rate=0.01)\n",
    "            model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    \n",
    "            early_stop = EarlyStopping(monitor='val_rmse', patience=5, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3)\n",
    "    \n",
    "            # Optional: speichern der besten Gewichte\n",
    "            #checkpoint = ModelCheckpoint(\"model/NN_model_flow_neighbour_best.keras\", monitor='val_rmse', save_best_only=True, mode='min')\n",
    "    \n",
    "            hist = model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                validation_split=0.2,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stop, reduce_lr],\n",
    "                verbose=0\n",
    "            )\n",
    "    \n",
    "            # Ergebnisse speichern\n",
    "            min_err = min(hist.history['rmse'])\n",
    "            min_val_err = min(hist.history['val_rmse'])\n",
    "            it_df = pd.DataFrame({\n",
    "                \"number_neurons\": [neurons],\n",
    "                \"number_layers\": [num_layer],\n",
    "                \"dropout_rate\":[dropout_rate],\n",
    "                \"train_rmse\": [min_err],\n",
    "                \"val_rmse\": [min_val_err]\n",
    "            })\n",
    "            grid_search_df = pd.concat([grid_search_df, it_df], axis=0)\n",
    "            print(neurons, num_layer,dropout_rate, \"done\")\n",
    "\n",
    "# Übersicht\n",
    "grid_search_df.reset_index(drop=True, inplace=True)\n",
    "print(grid_search_df)\n",
    "best_config = grid_search_df.loc[grid_search_df['val_rmse'].idxmin()]\n",
    "best_neurons = int(best_config['number_neurons'])\n",
    "best_layers = int(best_config['number_layers'])\n",
    "best_dropout = int(best_config['dropout_rate'])\n",
    "print(best_neurons, best_layers, best_dropout)\n",
    "\n",
    "# Modell mit besten Parametern\n",
    "best_model = Sequential()\n",
    "for _ in range(best_layers):\n",
    "    best_model.add(Dense(best_neurons, activation='relu'))\n",
    "    if best_dropout > 0:\n",
    "        best_model.add(Dropout(best_dropout))\n",
    "best_model.add(Dense(1))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "best_model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_rmse', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3)\n",
    "checkpoint = ModelCheckpoint(\"model/NN_model_flow_neighbour_best.keras\", monitor='val_rmse', save_best_only=True, mode='min')\n",
    "\n",
    "# Training\n",
    "best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Testbewertung\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best FLOW NN -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8a4e83b5-2ba0-47f7-b459-24e59f4989bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\AppData\\Local\\Temp\\ipykernel_10492\\326426236.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  grid_search_df = pd.concat([grid_search_df, it_df], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2 0.0 done\n",
      "16 2 0.05 done\n",
      "16 2 0.1 done\n",
      "16 3 0.0 done\n",
      "16 3 0.05 done\n",
      "16 3 0.1 done\n",
      "16 4 0.0 done\n",
      "16 4 0.05 done\n",
      "16 4 0.1 done\n",
      "32 2 0.0 done\n",
      "32 2 0.05 done\n",
      "32 2 0.1 done\n",
      "32 3 0.0 done\n",
      "32 3 0.05 done\n",
      "32 3 0.1 done\n",
      "32 4 0.0 done\n",
      "32 4 0.05 done\n",
      "32 4 0.1 done\n",
      "64 2 0.0 done\n",
      "64 2 0.05 done\n",
      "64 2 0.1 done\n",
      "64 3 0.0 done\n",
      "64 3 0.05 done\n",
      "64 3 0.1 done\n",
      "64 4 0.0 done\n",
      "64 4 0.05 done\n",
      "64 4 0.1 done\n",
      "128 2 0.0 done\n",
      "128 2 0.05 done\n",
      "128 2 0.1 done\n",
      "128 3 0.0 done\n",
      "128 3 0.05 done\n",
      "128 3 0.1 done\n",
      "128 4 0.0 done\n",
      "128 4 0.05 done\n",
      "128 4 0.1 done\n",
      "   number_neurons number_layers  dropout_rate  train_rmse  val_rmse\n",
      "0              16             2          0.00    0.917047  0.926511\n",
      "1              16             2          0.05    0.908428  0.900218\n",
      "2              16             2          0.10    0.936143  0.904106\n",
      "3              16             3          0.00    0.855502  0.891089\n",
      "4              16             3          0.05    0.913606  0.893798\n",
      "5              16             3          0.10    0.961896  0.917284\n",
      "6              16             4          0.00    0.858055  0.892206\n",
      "7              16             4          0.05    0.901608  0.892986\n",
      "8              16             4          0.10    0.953182  0.898682\n",
      "9              32             2          0.00    0.870039  0.893049\n",
      "10             32             2          0.05    1.013831  0.976710\n",
      "11             32             2          0.10    0.983026  0.973556\n",
      "12             32             3          0.00    0.881280  0.899969\n",
      "13             32             3          0.05    1.059069  0.964000\n",
      "14             32             3          0.10    0.890266  0.887247\n",
      "15             32             4          0.00    0.911515  0.947719\n",
      "16             32             4          0.05    1.002717  0.949513\n",
      "17             32             4          0.10    0.917669  0.901365\n",
      "18             64             2          0.00    0.865058  0.900217\n",
      "19             64             2          0.05    0.901713  0.884898\n",
      "20             64             2          0.10    0.873667  0.883873\n",
      "21             64             3          0.00    1.011136  0.965759\n",
      "22             64             3          0.05    0.973729  0.924550\n",
      "23             64             3          0.10    0.887519  0.887344\n",
      "24             64             4          0.00    0.955478  0.946387\n",
      "25             64             4          0.05    1.154971  1.002804\n",
      "26             64             4          0.10    0.923697  0.901611\n",
      "27            128             2          0.00    0.899512  0.908445\n",
      "28            128             2          0.05    1.017718  0.968171\n",
      "29            128             2          0.10    0.866816  0.877788\n",
      "30            128             3          0.00    1.006634  1.008220\n",
      "31            128             3          0.05    1.068124  1.003751\n",
      "32            128             3          0.10    0.906062  0.895106\n",
      "33            128             4          0.00    1.034620  0.973314\n",
      "34            128             4          0.05    1.148691  0.969545\n",
      "35            128             4          0.10    1.308806  1.123274\n",
      "128 2 0\n",
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step\n",
      "Best SPEED NN -> RMSE: 0.934, MAE: 0.437, R²: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#neighbour portal-speed\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_train[speed_features_neighbour])\n",
    "X_test_scaled  = scaler.transform(df_test[speed_features_neighbour])\n",
    "y_train = df_train[\"SPEED_future_mean\"]\n",
    "y_test = df_test[\"SPEED_future_mean\"]\n",
    "\n",
    "number_neurons = [16, 32, 64, 128]\n",
    "number_layers = [2, 3,4]\n",
    "dropout_rates = [0.0, 0.05, 0.1]\n",
    "\n",
    "grid_search_df = pd.DataFrame(columns=[\"number_neurons\",\"number_layers\",\"dropout_rate\",\"train_rmse\", \"val_rmse\"])\n",
    "\n",
    "for neurons in number_neurons:\n",
    "    for num_layer in number_layers:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            model = Sequential()\n",
    "            for _ in range(num_layer):\n",
    "                model.add(Dense(neurons, activation='relu'))\n",
    "                if dropout_rate > 0:\n",
    "                    model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(1))\n",
    "            optimizer = Adam(learning_rate=0.01)\n",
    "            model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    \n",
    "            early_stop = EarlyStopping(monitor='val_rmse', patience=5, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3)\n",
    "    \n",
    "            # Optional: speichern der besten Gewichte\n",
    "            #checkpoint = ModelCheckpoint(\"model/NN_model_speed_neighbour_best.keras\", monitor='val_rmse', save_best_only=True, mode='min')\n",
    "    \n",
    "            hist = model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                validation_split=0.2,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stop, reduce_lr],\n",
    "                verbose=0\n",
    "            )\n",
    "    \n",
    "            # Ergebnisse speichern\n",
    "            min_err = min(hist.history['rmse'])\n",
    "            min_val_err = min(hist.history['val_rmse'])\n",
    "            it_df = pd.DataFrame({\n",
    "                \"number_neurons\": [neurons],\n",
    "                \"number_layers\": [num_layer],\n",
    "                \"dropout_rate\":[dropout_rate],\n",
    "                \"train_rmse\": [min_err],\n",
    "                \"val_rmse\": [min_val_err]\n",
    "            })\n",
    "            grid_search_df = pd.concat([grid_search_df, it_df], axis=0)\n",
    "            print(neurons, num_layer,dropout_rate, \"done\")\n",
    "\n",
    "# Übersicht\n",
    "grid_search_df.reset_index(drop=True, inplace=True)\n",
    "print(grid_search_df)\n",
    "best_config = grid_search_df.loc[grid_search_df['val_rmse'].idxmin()]\n",
    "best_neurons = int(best_config['number_neurons'])\n",
    "best_layers = int(best_config['number_layers'])\n",
    "best_dropout = int(best_config['dropout_rate'])\n",
    "print(best_neurons, best_layers, best_dropout)\n",
    "\n",
    "# Modell mit besten Parametern\n",
    "best_model = Sequential()\n",
    "for _ in range(best_layers):\n",
    "    best_model.add(Dense(best_neurons, activation='relu'))\n",
    "    if best_dropout > 0:\n",
    "        best_model.add(Dropout(best_dropout))\n",
    "best_model.add(Dense(1))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "best_model.compile(optimizer=optimizer, loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_rmse', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3)\n",
    "checkpoint = ModelCheckpoint(\"model/NN_model_speed_neighbour_best.keras\", monitor='val_rmse', save_best_only=True, mode='min')\n",
    "\n",
    "# Training\n",
    "best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Testbewertung\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best SPEED NN -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "770e6df0-3513-4d4a-8f5e-8bea1040e430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d41bb8c-c0c6-4c9c-b240-9daeaac404bc",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eac6fea4-f5c7-44ca-b9f3-81dba385d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "flow_features_same = [\n",
    "    f'SENSOR_{sensor}_FLOW'\n",
    "    for sensor in same_sensors\n",
    "    #for i in range(nlags)\n",
    "]\n",
    "flow_features_neighbour = [\n",
    "    f'SENSOR_{sensor}_FLOW'\n",
    "    for sensor in neighbour_sensors\n",
    "    #for i in range(nlags)\n",
    "]\n",
    "speed_features_same = [\n",
    "    f'SENSOR_{sensor}_SPEED'\n",
    "    for sensor in same_sensors\n",
    "    #for i in range(nlags)\n",
    "]\n",
    "speed_features_neighbour = [\n",
    "    f'SENSOR_{sensor}_SPEED'\n",
    "    for sensor in neighbour_sensors\n",
    "    #for i in range(nlags)\n",
    "]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b908c02c-d444-49fa-8820-dc973651104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, seq_length=15, horizon=15):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_length - horizon + 1):\n",
    "        Xs.append(X[i:i+seq_length])\n",
    "        ys.append(y[i+seq_length:i+seq_length+horizon])\n",
    "    return np.array(Xs), np.array(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "65d84553-3e22-40d2-b77c-50fbab086b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = df_all[flow_features_neighbour].values\n",
    "y = df_all[f'SENSOR_{target_sensor}_FLOW'].values\n",
    "\n",
    "\n",
    "X_seq, y_seq = create_sequences(X, y, seq_length=15, horizon=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "201fcd02-866e-47db-afec-c7f09e80d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "n_features = X_seq.shape[2]\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='tanh', input_shape=(15, n_features)),\n",
    "    Dense(15)  # 15 Output-Werte für 15-Minuten-Vorhersage\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10bef158-00e8-4da8-970e-219f1c8638cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 30.0420 - mae: 3.9377 - val_loss: 18.6247 - val_mae: 3.1603\n",
      "Epoch 2/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 16.4002 - mae: 3.0792 - val_loss: 18.7190 - val_mae: 3.1875\n",
      "Epoch 3/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 16.2702 - mae: 3.0620 - val_loss: 18.6891 - val_mae: 3.1704\n",
      "Epoch 4/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 16.1985 - mae: 3.0520 - val_loss: 18.9714 - val_mae: 3.1972\n",
      "Epoch 5/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 16.1668 - mae: 3.0470 - val_loss: 18.7135 - val_mae: 3.1746\n",
      "Epoch 6/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 16.1104 - mae: 3.0407 - val_loss: 18.7539 - val_mae: 3.1704\n",
      "Epoch 7/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 16.0740 - mae: 3.0354 - val_loss: 18.6065 - val_mae: 3.1640\n",
      "Epoch 8/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 16.0453 - mae: 3.0322 - val_loss: 18.7395 - val_mae: 3.1733\n",
      "Epoch 9/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 16.0244 - mae: 3.0302 - val_loss: 18.6445 - val_mae: 3.1651\n",
      "Epoch 10/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 15.9952 - mae: 3.0257 - val_loss: 18.7595 - val_mae: 3.1709\n",
      "Epoch 11/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 15.9606 - mae: 3.0218 - val_loss: 18.7141 - val_mae: 3.1737\n",
      "Epoch 12/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 15.9529 - mae: 3.0200 - val_loss: 18.6432 - val_mae: 3.1751\n",
      "Epoch 13/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 15.9099 - mae: 3.0148 - val_loss: 18.4972 - val_mae: 3.1701\n",
      "Epoch 14/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 15.8939 - mae: 3.0128 - val_loss: 18.4324 - val_mae: 3.1819\n",
      "Epoch 15/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 15.8926 - mae: 3.0128 - val_loss: 18.5521 - val_mae: 3.1801\n",
      "Epoch 16/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 15.8597 - mae: 3.0091 - val_loss: 18.4691 - val_mae: 3.1713\n",
      "Epoch 17/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 15.8396 - mae: 3.0069 - val_loss: 18.5678 - val_mae: 3.1751\n",
      "Epoch 18/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 15.8196 - mae: 3.0046 - val_loss: 18.5583 - val_mae: 3.1796\n",
      "Epoch 19/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 15.8032 - mae: 3.0028 - val_loss: 18.3414 - val_mae: 3.1497\n",
      "Epoch 20/20\n",
      "\u001b[1m1714/1714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 15.7829 - mae: 3.0011 - val_loss: 18.6377 - val_mae: 3.1854\n",
      "\u001b[1m476/476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1,verbose=1)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8f1d00d5-fbab-4589-a70d-baa3b4ef175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM (FLOW) -> RMSE: 3.896, MAE: 2.855, R2: 0.651\n",
      "LSTM Sum Forecast (FLOW) → RMSE: 34.495, MAE: 21.517, R²: 0.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Johanna\\anaconda3\\envs\\AI_project\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"LSTM (FLOW) -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, R2: {r2:.3f}\")\n",
    "\n",
    "y_test_sum = y_test.sum(axis=1)\n",
    "y_pred_sum = y_pred.sum(axis=1)\n",
    "\n",
    "# Metriken berechnen\n",
    "rmse = mean_squared_error(y_test_sum, y_pred_sum, squared=False)\n",
    "mae = mean_absolute_error(y_test_sum, y_pred_sum)\n",
    "r2 = r2_score(y_test_sum, y_pred_sum)\n",
    "\n",
    "print(f\"LSTM Sum Forecast (FLOW) → RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aeb5b2-c985-47cb-9599-d777b013feaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
